
# ğŸ“Š Netflix Data Engineering Pipeline on Azure

This project demonstrates how to build an end-to-end data ingestion and processing pipeline using Microsoft Azure services. It ingests Netflix dataset CSVs from GitHub and processes them into a layered Delta Lake architecture using Azure Data Factory, Azure Data Lake Storage Gen2, and Azure Databricks with Autoloader.

---

## ğŸ’¡ Project Overview

This pipeline is designed to:

- Automatically ingest CSV files from a GitHub repository.
- Store raw files in a hierarchical Azure Data Lake (Raw â†’ Bronze â†’ Silver â†’ Gold).
- Use Azure Data Factory for dynamic ingestion using parameterized pipelines.
- Transform and stream files using Azure Databricks Autoloader.
- Structure data using the Delta Lake format for scalable querying.

---

## ğŸ”§ Technologies Used

- Azure Resource Group
- Azure Data Lake Storage Gen2
- Azure Data Factory (ADF)
- Azure Databricks
- Delta Lake
- Autoloader
- GitHub (as source system)

---

## ğŸ—‚ï¸ Architecture Overview

```
GitHub Repo (CSV Files)
        â†“
Azure Data Factory (ADF)
  - Parameterized pipeline
  - HTTP source & ForEach loop
        â†“
Azure Data Lake Gen2
  - Containers: raw, bronze, silver, gold
        â†“
Azure Databricks Autoloader
  - Streams from raw to bronze
  - Transforms to silver and gold
```

---

## ğŸ› ï¸ Step-by-Step Setup

### 1. Azure Setup

- Create a **Resource Group**.
- Create a **Storage Account** with **Hierarchical Namespace** enabled.
- Create four containers:
  - `raw`, `bronze`, `silver`, `gold`

### 2. Azure Data Factory (ADF)

- Create two **Linked Services**:
  - HTTP (GitHub): `https://raw.githubusercontent.com/`
  - Azure Data Lake Storage Gen2 (connect to your storage account)

### 3. ADF Pipeline

- Create **Parameters** for folder and file names.
- Add **Copy Data** activity with:
  - Source: HTTP (GitHub)
  - Sink: Azure Data Lake (Raw container)
- Use a **ForEach** activity:
  - Loop through an array of JSON file objects.
  - Example:

```json
[
  { "folder_name": "netflix_cast", "file_name": "netflix_cast.csv" },
  { "folder_name": "netflix_category", "file_name": "netflix_category.csv" },
  ...
]
```

- Add a **Web Activity** (optional) to fetch metadata or validate files.
- Set variables as needed for dynamic behavior.

### 4. Azure Databricks Autoloader

```python
checkpoint_location = "abfss://silver@<your-storage>.dfs.core.windows.net/checkpoint"

df = spark.readStream  .format("cloudFiles")  .option("cloudFiles.format", "csv")  .option("cloudFiles.schemaLocation", checkpoint_location)  .load("abfss://raw@<your-storage>.dfs.core.windows.net")

df.writeStream  .option("checkpointLocation", checkpoint_location)  .trigger(processingTime='10 seconds')  .start("abfss://bronze@<your-storage>.dfs.core.windows.net/netflix_titles")
```

- Autoloader monitors new files and streams them into Bronze.
- Use follow-up notebooks for transformations from Bronze â†’ Silver â†’ Gold using Delta format.

---

## ğŸ“ Folder Structure

```
netflix-data-pipeline-azure/
â”œâ”€â”€ adf-pipeline/
â”‚   â””â”€â”€ pipeline-definition.json
â”œâ”€â”€ databricks-notebooks/
â”‚   â”œâ”€â”€ autoloader.py
â”‚   â”œâ”€â”€ silver_transform.py
â”‚   â””â”€â”€ gold_transform.py
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ netflix_cast.csv
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture-diagram.png
â””â”€â”€ README.md
```

---

## ğŸ§  Key Learnings

- Cloud-native data pipelines with ADF and Databricks
- Parameterized and dynamic data ingestion
- Delta Lake architecture for modern data warehousing
- Streaming data ingestion with Autoloader
- Layered data lake design (Raw â†’ Bronze â†’ Silver â†’ Gold)

---

## ğŸ Final Result

A scalable Azure-based data pipeline capable of automatically ingesting, storing, and transforming data from external sources, following best practices for modern data engineering.

---

## ğŸ“¬ Contact

Built by [Your Name]. Feel free to connect with me on [LinkedIn] or check out more of my work on [GitHub].
