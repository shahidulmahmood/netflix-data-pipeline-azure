
---

## ğŸ”§ Setup Steps

### 1ï¸âƒ£ Create Linked Services

#### ğŸ”¹ HTTP Linked Service (GitHub)
- Go to **Manage** tab > **Linked Services** > **+ New**
- Choose **HTTP**
- Set:
  - **Name**: `GitHub_LinkedService`
  - **Base URL**: `https://raw.githubusercontent.com/`
  - **Authentication**: Anonymous (for public repos)
- Click **Create**

#### ğŸ”¹ Azure Data Lake Gen2 Linked Service
- Go to **Linked Services** > **+ New**
- Choose **Azure Data Lake Storage Gen2**
- Set:
  - **Name**: `DataLake_LinkedService`
  - Choose your storage account
- Click **Create**

---

### 2ï¸âƒ£ Create Pipeline with Parameters

- Create two parameters in your pipeline:
  - `folder_name`
  - `file_name`

---

### 3ï¸âƒ£ Set Up Datasets

#### ğŸ”¹ HTTP Dataset (GitHub)
- Linked Service: `GitHub_LinkedService`
- Relative URL: dynamic
  - Use:
    ```json
    @concat('<user>/<repo>/main/', pipeline().parameters.folder_name, '/', pipeline().parameters.file_name)
    ```

#### ğŸ”¹ Azure Data Lake Dataset
- Linked Service: `DataLake_LinkedService`
- File path: dynamic
  - Use:
    ```json
    @concat(pipeline().parameters.folder_name, '/', pipeline().parameters.file_name)
    ```

---

### 4ï¸âƒ£ Create a ForEach Activity

- Input: an array of JSON like this:

```json
[
  { "folder_name": "netflix_cast", "file_name": "netflix_cast.csv" },
  { "folder_name": "netflix_category", "file_name": "netflix_category.csv" }
]
```

- Inside the loop:
  - Use **Copy Data** activity
  - Map parameters:
    - `@item().folder_name`
    - `@item().file_name`

---

## ğŸ§  Summary

| Step | Purpose |
|------|---------|
| HTTP Linked Service | Connect ADF to GitHub |
| Data Lake Linked Service | Enable write access to ADLS |
| Parameters | Make ingestion dynamic |
| ForEach | Loop through multiple datasets |
| Copy Data | Perform file transfer from GitHub to ADLS |

---

## âœ… Outcome

A fully reusable and scalable ADF pipeline that:
- Connects to GitHub without hardcoding file paths
- Dynamically ingests multiple files with one flow
- Cleanly organizes raw files in your Azure Data Lake

---

## ğŸ—ï¸ Folder Structure

```
adf-github-ingestion/
â”œâ”€â”€ adf/
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ linkedServices/
â”œâ”€â”€ README.md
```

---

## âœï¸ Author

Built by [Your Name].  
Let me know if you'd like help creating a downloadable JSON template of this pipeline.
